{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain_community langchain_huggingface langchain_groq\n",
    "!pip install pypdf faiss-gpu-cu11 groq gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:30:34.311027Z",
     "iopub.status.busy": "2025-02-17T20:30:34.310429Z",
     "iopub.status.idle": "2025-02-17T20:30:34.315951Z",
     "shell.execute_reply": "2025-02-17T20:30:34.315025Z",
     "shell.execute_reply.started": "2025-02-17T20:30:34.311001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:30:35.966728Z",
     "iopub.status.busy": "2025-02-17T20:30:35.966310Z",
     "iopub.status.idle": "2025-02-17T20:30:35.970902Z",
     "shell.execute_reply": "2025-02-17T20:30:35.970103Z",
     "shell.execute_reply.started": "2025-02-17T20:30:35.966693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"GROQ_API_KEY\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"TAVILY_API_KEY\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"LANGSMITH_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "PDF_FILE_PATH = \"/kaggle/input/demo-pdf-rag/John_Doe_Machine_Learning_Engineer_Resume_v2.pdf\"\n",
    "EMBEDDING_MODEL = HuggingFaceEmbeddings()\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:31:35.986232Z",
     "iopub.status.busy": "2025-02-17T20:31:35.985770Z",
     "iopub.status.idle": "2025-02-17T20:31:35.990113Z",
     "shell.execute_reply": "2025-02-17T20:31:35.989324Z",
     "shell.execute_reply.started": "2025-02-17T20:31:35.986193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "def initialize_llm():\n",
    "    return ChatGroq(\n",
    "        model=\"deepseek-r1-distill-llama-70b\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        streaming=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:31:36.787028Z",
     "iopub.status.busy": "2025-02-17T20:31:36.786773Z",
     "iopub.status.idle": "2025-02-17T20:31:37.865592Z",
     "shell.execute_reply": "2025-02-17T20:31:37.864706Z",
     "shell.execute_reply.started": "2025-02-17T20:31:36.787009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:31:37.866730Z",
     "iopub.status.busy": "2025-02-17T20:31:37.866516Z",
     "iopub.status.idle": "2025-02-17T20:31:37.870420Z",
     "shell.execute_reply": "2025-02-17T20:31:37.869473Z",
     "shell.execute_reply.started": "2025-02-17T20:31:37.866712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a Machine Learning expert\",\n",
    "    ),\n",
    "    (\"human\", \"give me a list of Machine Learning models.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:31:39.310677Z",
     "iopub.status.busy": "2025-02-17T20:31:39.310306Z",
     "iopub.status.idle": "2025-02-17T20:31:46.871534Z",
     "shell.execute_reply": "2025-02-17T20:31:46.870817Z",
     "shell.execute_reply.started": "2025-02-17T20:31:39.310648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The landscape of machine learning models is diverse, encompassing various approaches tailored to different data scenarios and tasks. Here's an organized overview of the key categories and models:\n",
      "\n",
      "### 1. Supervised Learning\n",
      "- **Definition**: Models are trained on labeled data, where each example is paired with the correct output.\n",
      "- **Models**:\n",
      "  - **Decision Trees**: Flowchart-like models for decision-making.\n",
      "  - **Linear Regression**: Predicts continuous outcomes.\n",
      "  - **Logistic Regression**: Used for binary classification.\n",
      "  - **Support Vector Machines (SVMs)**: Find hyperplanes to separate classes.\n",
      "  - **K-Nearest Neighbors (KNN)**: Predicts based on nearest data points.\n",
      "  - **Naive Bayes**: Probabilistic models for classification.\n",
      "  - **Random Forests and Gradient-Boosted Trees**: Ensemble methods for improved performance.\n",
      "  - **Neural Networks**: Can be used for both classification and regression.\n",
      "\n",
      "### 2. Unsupervised Learning\n",
      "- **Definition**: Models find patterns in unlabeled data.\n",
      "- **Models**:\n",
      "  - **K-Means**: Clusters data into groups.\n",
      "  - **Principal Component Analysis (PCA)**: Reduces data dimensionality.\n",
      "  - **t-SNE**: Visualizes high-dimensional data.\n",
      "  - **DBSCAN**: Density-based clustering.\n",
      "  - **Gaussian Mixture Models (GMMs)**: Probabilistic clustering.\n",
      "  - **Anomaly Detection (e.g., Isolation Forest)**: Identifies outliers.\n",
      "  - **Association Rule Learning (e.g., Apriori)**: Finds frequent item sets.\n",
      "\n",
      "### 3. Semi-Supervised Learning\n",
      "- **Definition**: Combines labeled and unlabeled data.\n",
      "- **Models**:\n",
      "  - **Generative Adversarial Networks (GANs)**: Generate data through competition.\n",
      "  - **Self-Supervised Learning**: Uses data to generate labels.\n",
      "  - **Transfer Learning**: Leverages pre-trained models for new tasks.\n",
      "\n",
      "### 4. Reinforcement Learning\n",
      "- **Definition**: Agents learn via trial and error, receiving rewards or penalties.\n",
      "- **Models**:\n",
      "  - **Q-Learning**: Basic algorithm for action value learning.\n",
      "  - **Deep Q-Networks (DQN)**: Combines Q-learning with neural networks.\n",
      "  - **Policy Gradient Methods**: Direct policy optimization.\n",
      "  - **Actor-Critic Methods**: Combines policy and value-based approaches.\n",
      "  - **Proximal Policy Optimization (PPO)**: Advanced for continuous control.\n",
      "\n",
      "### 5. Deep Learning\n",
      "- **Definition**: Uses neural networks with multiple layers for complex tasks.\n",
      "- **Models**:\n",
      "  - **Convolutional Neural Networks (CNNs)**: Image processing.\n",
      "  - **Recurrent Neural Networks (RNNs)**: Sequential data (e.g., text, speech).\n",
      "  - **Transformers**: NLP tasks, with models like BERT and GPT.\n",
      "  - **Autoencoders**: Data compression and reconstruction.\n",
      "  - **Generative Models (e.g., GANs, VAEs)**: Generate new data.\n",
      "\n",
      "### 6. Specialized Models\n",
      "- **Recommendation Systems**: Suggest items based on behavior (e.g., Collaborative Filtering).\n",
      "- **Time Series Models (e.g., ARIMA)**: Forecast future values.\n",
      "- **NLP Models**: Include bag-of-words, TF-IDF, Word2Vec, and GloVe.\n",
      "\n",
      "### Clarifications\n",
      "- **Neural Networks**: Versatile across supervised and unsupervised tasks.\n",
      "- **GANs**: Fit both unsupervised and semi-supervised contexts.\n",
      "- **Transfer Learning**: A technique, often used with deep learning.\n",
      "\n",
      "This structured approach helps in understanding the application and context of each model, aiding in selecting the right tool for specific machine learning tasks.\n"
     ]
    }
   ],
   "source": [
    "output = llm.invoke(messages).content\n",
    "# remove the \"thinking\" part from response\n",
    "output = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:15.376128Z",
     "iopub.status.busy": "2025-02-17T20:32:15.375857Z",
     "iopub.status.idle": "2025-02-17T20:32:15.379659Z",
     "shell.execute_reply": "2025-02-17T20:32:15.378885Z",
     "shell.execute_reply.started": "2025-02-17T20:32:15.376107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize search tool\n",
    "def initialize_search_tool():\n",
    "    return TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:15.895955Z",
     "iopub.status.busy": "2025-02-17T20:32:15.895746Z",
     "iopub.status.idle": "2025-02-17T20:32:15.900536Z",
     "shell.execute_reply": "2025-02-17T20:32:15.899696Z",
     "shell.execute_reply.started": "2025-02-17T20:32:15.895936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize retriever tool\n",
    "def initialize_retriever_tool(file_path):\n",
    "    \"\"\"\n",
    "    This function initializes a retriever tool using a PDF document provided\n",
    "    by the `file_path`. The PDF is loaded, processed into smaller chunks, \n",
    "    and then converted into a vector representation using FAISS. \n",
    "    The retriever tool is then created for searching information related to\n",
    "    John Doe's resume.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file containing the resume data.\n",
    "\n",
    "    Returns:\n",
    "        RetrieverTool or None: The retriever tool if the file exists and is processed successfully, \n",
    "                                or None if the file path is invalid or the file does not exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file path is valid and if the file exists\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        \n",
    "        # Load the PDF file from the specified file path\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()  # Extracts the document content from the PDF\n",
    "        \n",
    "        # Split the loaded document into smaller chunks for processing\n",
    "        # This is done to manage the document size and improve search performance\n",
    "        documents = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "        ).split_documents(docs)\n",
    "\n",
    "        # Create a FAISS vector from the document chunks using the specified embedding model\n",
    "        vector = FAISS.from_documents(documents, EMBEDDING_MODEL)\n",
    "        \n",
    "        # Create a retriever from the FAISS vector for searching document content\n",
    "        retriever = vector.as_retriever()\n",
    "\n",
    "        # Return the retriever tool, with a description of its purpose\n",
    "        return create_retriever_tool(\n",
    "            retriever,\n",
    "            \"resume_search\",  # Tool name for identifying this retriever\n",
    "            \"Search for information about John Doe's resume. For any questions about his qualifications, experience, and skills, use this tool!\",  # Tool description\n",
    "        )\n",
    "    \n",
    "    # Return None if the file doesn't exist or the path is invalid\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:16.594558Z",
     "iopub.status.busy": "2025-02-17T20:32:16.594199Z",
     "iopub.status.idle": "2025-02-17T20:32:16.599311Z",
     "shell.execute_reply": "2025-02-17T20:32:16.598415Z",
     "shell.execute_reply.started": "2025-02-17T20:32:16.594529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize agent executor with better readability\n",
    "def initialize_agent_executor(llm, tools, logs: bool):\n",
    "    \"\"\"\n",
    "    Initialize the agent executor with the provided LLM, tools, and logging configuration.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to be used.\n",
    "        tools: The tools to be used by the agent.\n",
    "        logs (bool): Whether to enable logging.\n",
    "\n",
    "    Returns:\n",
    "        AgentExecutor: The initialized agent executor.\n",
    "    \"\"\"\n",
    "    # Pull the prompt for the agent\n",
    "    prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "    \n",
    "    # Create the agent using the provided LLM and tools\n",
    "    agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "    \n",
    "    # Return the initialized AgentExecutor with the logging configuration\n",
    "    return AgentExecutor(agent=agent, tools=tools, verbose=logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:17.226327Z",
     "iopub.status.busy": "2025-02-17T20:32:17.226064Z",
     "iopub.status.idle": "2025-02-17T20:32:17.230101Z",
     "shell.execute_reply": "2025-02-17T20:32:17.229455Z",
     "shell.execute_reply.started": "2025-02-17T20:32:17.226307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def initialize_tools(file_path):\n",
    "    \"\"\"Initializes and returns LLM, search tool, and retriever tool.\"\"\"\n",
    "    llm = initialize_llm()\n",
    "    search_tool = initialize_search_tool()\n",
    "    retriever_tool = initialize_retriever_tool(file_path)\n",
    "    \n",
    "    # Ensure tools list does not contain None values\n",
    "    tools = [search_tool, retriever_tool] if retriever_tool else [search_tool]\n",
    "    \n",
    "    return llm, tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:17.907590Z",
     "iopub.status.busy": "2025-02-17T20:32:17.907306Z",
     "iopub.status.idle": "2025-02-17T20:32:17.911639Z",
     "shell.execute_reply": "2025-02-17T20:32:17.910781Z",
     "shell.execute_reply.started": "2025-02-17T20:32:17.907567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_chat_history(chat_history):\n",
    "    \"\"\"Formats chat history into a string suitable for model input.\"\"\"\n",
    "    chat_history = \"\\n\".join(\n",
    "        [f\"{msg['role']}: {msg['content']}\" for msg in chat_history]\n",
    "    )\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:18.716940Z",
     "iopub.status.busy": "2025-02-17T20:32:18.716731Z",
     "iopub.status.idle": "2025-02-17T20:32:18.720803Z",
     "shell.execute_reply": "2025-02-17T20:32:18.719981Z",
     "shell.execute_reply.started": "2025-02-17T20:32:18.716922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_agent_response(agent_executor, chat_history):\n",
    "    \"\"\"Invokes the agent executor with formatted chat history and returns the response.\"\"\"\n",
    "    formatted_history = format_chat_history(chat_history)\n",
    "    \n",
    "    response_data = agent_executor.invoke(\n",
    "        {\"input\": formatted_history},\n",
    "        config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    "    )\n",
    "\n",
    "    return response_data.get(\"output\", \"\")  # Handle None response safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:19.397675Z",
     "iopub.status.busy": "2025-02-17T20:32:19.397465Z",
     "iopub.status.idle": "2025-02-17T20:32:19.401485Z",
     "shell.execute_reply": "2025-02-17T20:32:19.400769Z",
     "shell.execute_reply.started": "2025-02-17T20:32:19.397656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def stream_response(bot_response, chat_history):\n",
    "    \"\"\"Streams the bot's response character by character, yielding updates.\"\"\"\n",
    "    global stop_generation\n",
    "    \n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": \"\"})  # Placeholder for response\n",
    "    \n",
    "    for char in bot_response:\n",
    "        if stop_generation:\n",
    "            break\n",
    "        chat_history[-1][\"content\"] += char\n",
    "        yield \"\", chat_history  # Yield updated chat history for real-time display\n",
    "        time.sleep(0.05)  # Simulate a typing delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:20.344539Z",
     "iopub.status.busy": "2025-02-17T20:32:20.344253Z",
     "iopub.status.idle": "2025-02-17T20:32:20.348899Z",
     "shell.execute_reply": "2025-02-17T20:32:20.348058Z",
     "shell.execute_reply.started": "2025-02-17T20:32:20.344517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def response(message, chat_history, file_path, max_messages=10):\n",
    "    \"\"\"\n",
    "    Handles user input, generates a response from the chatbot agent, and streams it character by character.\n",
    "\n",
    "    Parameters:\n",
    "        message (str): The user's input message to the chatbot.\n",
    "        chat_history (list): The conversation history as a list of dictionaries.\n",
    "        max_messages (int): Maximum number of messages to keep in history.\n",
    "\n",
    "    Yields:\n",
    "        tuple: An empty string (reserved for additional outputs) and the updated chat history.\n",
    "    \"\"\"\n",
    "    global stop_generation\n",
    "    stop_generation = False  # Reset stop flag\n",
    "\n",
    "    # Initialize LLM and tools\n",
    "    llm, tools = initialize_tools(file_path)\n",
    "    agent_executor = initialize_agent_executor(llm, tools, False)\n",
    "\n",
    "    # Limit chat history size\n",
    "    chat_history[:] = chat_history[-max_messages:] \n",
    "\n",
    "    # Add user message to chat history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Get bot response\n",
    "    bot_response = get_agent_response(agent_executor, chat_history)\n",
    "\n",
    "    # Stream response character by character\n",
    "    yield from stream_response(bot_response, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:21.618811Z",
     "iopub.status.busy": "2025-02-17T20:32:21.618528Z",
     "iopub.status.idle": "2025-02-17T20:32:21.622424Z",
     "shell.execute_reply": "2025-02-17T20:32:21.621539Z",
     "shell.execute_reply.started": "2025-02-17T20:32:21.618789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def stop_generation_fn():\n",
    "  \"\"\"\n",
    "  Sets the `stop_generation` flag to True, signaling the response function to halt generation.\n",
    "\n",
    "  This function is typically triggered by a user action (e.g., pressing a \"Stop\" button).\n",
    "  \"\"\"\n",
    "  global stop_generation\n",
    "  stop_generation = True  # Signal to stop the response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:24.319501Z",
     "iopub.status.busy": "2025-02-17T20:32:24.319155Z",
     "iopub.status.idle": "2025-02-17T20:32:24.497565Z",
     "shell.execute_reply": "2025-02-17T20:32:24.496940Z",
     "shell.execute_reply.started": "2025-02-17T20:32:24.319471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Origin(radius_size=gr.themes.sizes.radius_md)) as demo:\n",
    "  gr.Markdown(\"# Chat Bot\")\n",
    "  chatbot = gr.Chatbot(type=\"messages\")\n",
    "  msg = gr.Textbox(label=\"Prompt\")\n",
    "  with gr.Row():\n",
    "      with gr.Column(scale=2):\n",
    "          btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "          gr.ClearButton(components=[msg, chatbot], value=\"Clear\", variant=\"secondary\")\n",
    "          btn_stop = gr.Button(\"Stop\", variant=\"stop\")\n",
    "      with gr.Column(scale=2):\n",
    "          uploaded_file = gr.File(type=\"filepath\")\n",
    "          \n",
    "  btn.click(fn=response, inputs=[msg, chatbot, uploaded_file], outputs=[msg, chatbot])\n",
    "  msg.submit(fn=response, inputs=[msg, chatbot, uploaded_file], outputs=[msg, chatbot])\n",
    "  btn_stop.click(fn=stop_generation_fn, inputs=[], outputs=[])\n",
    "\n",
    "  gr.Examples(\n",
    "    examples=[\"What are John Doe's skills?\",\n",
    "              \"What is your name?\",\n",
    "              \"What is the current exchange rate of the dollar in Iran?\"\n",
    "             ],\n",
    "    inputs=[msg]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:32:25.706042Z",
     "iopub.status.busy": "2025-02-17T20:32:25.705782Z",
     "iopub.status.idle": "2025-02-17T20:32:28.839393Z",
     "shell.execute_reply": "2025-02-17T20:32:28.838630Z",
     "shell.execute_reply.started": "2025-02-17T20:32:25.706021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a96be048c0ae5e5b1a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a96be048c0ae5e5b1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.close_all()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T19:25:05.904286Z",
     "iopub.status.busy": "2025-02-17T19:25:05.903966Z",
     "iopub.status.idle": "2025-02-17T19:25:06.036267Z",
     "shell.execute_reply": "2025-02-17T19:25:06.035409Z",
     "shell.execute_reply.started": "2025-02-17T19:25:05.904265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6679070,
     "sourceId": 10766940,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
