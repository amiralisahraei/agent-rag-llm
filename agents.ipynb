{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10766940,"sourceType":"datasetVersion","datasetId":6679070}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain_community langchain_huggingface langchain_groq\n!pip install pypdf faiss-gpu-cu11 groq gradio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport time\nfrom langchain_groq import ChatGroq\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain import hub\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain.agents import AgentExecutor\n\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:30:34.310429Z","iopub.execute_input":"2025-02-17T20:30:34.311027Z","iopub.status.idle":"2025-02-17T20:30:34.315951Z","shell.execute_reply.started":"2025-02-17T20:30:34.311001Z","shell.execute_reply":"2025-02-17T20:30:34.315025Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ[\"GROQ_API_KEY\"] = \"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\"\nos.environ[\"TAVILY_API_KEY\"] = \"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\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"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\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:30:35.966310Z","iopub.execute_input":"2025-02-17T20:30:35.966728Z","iopub.status.idle":"2025-02-17T20:30:35.970902Z","shell.execute_reply.started":"2025-02-17T20:30:35.966693Z","shell.execute_reply":"2025-02-17T20:30:35.970103Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Constants\nPDF_FILE_PATH = \"/kaggle/input/demo-pdf-rag/John_Doe_Machine_Learning_Engineer_Resume_v2.pdf\"\nEMBEDDING_MODEL = HuggingFaceEmbeddings()\nCHUNK_SIZE = 1000\nCHUNK_OVERLAP = 200","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize LLM\ndef initialize_llm():\n    return ChatGroq(\n        model=\"deepseek-r1-distill-llama-70b\",\n        temperature=0.5,\n        max_tokens=None,\n        timeout=None,\n        streaming=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:31:35.985770Z","iopub.execute_input":"2025-02-17T20:31:35.986232Z","iopub.status.idle":"2025-02-17T20:31:35.990113Z","shell.execute_reply.started":"2025-02-17T20:31:35.986193Z","shell.execute_reply":"2025-02-17T20:31:35.989324Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"llm = initialize_llm()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:31:36.786773Z","iopub.execute_input":"2025-02-17T20:31:36.787028Z","iopub.status.idle":"2025-02-17T20:31:37.865592Z","shell.execute_reply.started":"2025-02-17T20:31:36.787009Z","shell.execute_reply":"2025-02-17T20:31:37.864706Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"messages = [\n    (\n        \"system\",\n        \"You are a Machine Learning expert\",\n    ),\n    (\"human\", \"give me a list of Machine Learning models.\"),\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:31:37.866516Z","iopub.execute_input":"2025-02-17T20:31:37.866730Z","iopub.status.idle":"2025-02-17T20:31:37.870420Z","shell.execute_reply.started":"2025-02-17T20:31:37.866712Z","shell.execute_reply":"2025-02-17T20:31:37.869473Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"output = llm.invoke(messages).content\n# remove the \"thinking\" part from response\noutput = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:31:39.310306Z","iopub.execute_input":"2025-02-17T20:31:39.310677Z","iopub.status.idle":"2025-02-17T20:31:46.871534Z","shell.execute_reply.started":"2025-02-17T20:31:39.310648Z","shell.execute_reply":"2025-02-17T20:31:46.870817Z"}},"outputs":[{"name":"stdout","text":"The landscape of machine learning models is diverse, encompassing various approaches tailored to different data scenarios and tasks. Here's an organized overview of the key categories and models:\n\n### 1. Supervised Learning\n- **Definition**: Models are trained on labeled data, where each example is paired with the correct output.\n- **Models**:\n  - **Decision Trees**: Flowchart-like models for decision-making.\n  - **Linear Regression**: Predicts continuous outcomes.\n  - **Logistic Regression**: Used for binary classification.\n  - **Support Vector Machines (SVMs)**: Find hyperplanes to separate classes.\n  - **K-Nearest Neighbors (KNN)**: Predicts based on nearest data points.\n  - **Naive Bayes**: Probabilistic models for classification.\n  - **Random Forests and Gradient-Boosted Trees**: Ensemble methods for improved performance.\n  - **Neural Networks**: Can be used for both classification and regression.\n\n### 2. Unsupervised Learning\n- **Definition**: Models find patterns in unlabeled data.\n- **Models**:\n  - **K-Means**: Clusters data into groups.\n  - **Principal Component Analysis (PCA)**: Reduces data dimensionality.\n  - **t-SNE**: Visualizes high-dimensional data.\n  - **DBSCAN**: Density-based clustering.\n  - **Gaussian Mixture Models (GMMs)**: Probabilistic clustering.\n  - **Anomaly Detection (e.g., Isolation Forest)**: Identifies outliers.\n  - **Association Rule Learning (e.g., Apriori)**: Finds frequent item sets.\n\n### 3. Semi-Supervised Learning\n- **Definition**: Combines labeled and unlabeled data.\n- **Models**:\n  - **Generative Adversarial Networks (GANs)**: Generate data through competition.\n  - **Self-Supervised Learning**: Uses data to generate labels.\n  - **Transfer Learning**: Leverages pre-trained models for new tasks.\n\n### 4. Reinforcement Learning\n- **Definition**: Agents learn via trial and error, receiving rewards or penalties.\n- **Models**:\n  - **Q-Learning**: Basic algorithm for action value learning.\n  - **Deep Q-Networks (DQN)**: Combines Q-learning with neural networks.\n  - **Policy Gradient Methods**: Direct policy optimization.\n  - **Actor-Critic Methods**: Combines policy and value-based approaches.\n  - **Proximal Policy Optimization (PPO)**: Advanced for continuous control.\n\n### 5. Deep Learning\n- **Definition**: Uses neural networks with multiple layers for complex tasks.\n- **Models**:\n  - **Convolutional Neural Networks (CNNs)**: Image processing.\n  - **Recurrent Neural Networks (RNNs)**: Sequential data (e.g., text, speech).\n  - **Transformers**: NLP tasks, with models like BERT and GPT.\n  - **Autoencoders**: Data compression and reconstruction.\n  - **Generative Models (e.g., GANs, VAEs)**: Generate new data.\n\n### 6. Specialized Models\n- **Recommendation Systems**: Suggest items based on behavior (e.g., Collaborative Filtering).\n- **Time Series Models (e.g., ARIMA)**: Forecast future values.\n- **NLP Models**: Include bag-of-words, TF-IDF, Word2Vec, and GloVe.\n\n### Clarifications\n- **Neural Networks**: Versatile across supervised and unsupervised tasks.\n- **GANs**: Fit both unsupervised and semi-supervised contexts.\n- **Transfer Learning**: A technique, often used with deep learning.\n\nThis structured approach helps in understanding the application and context of each model, aiding in selecting the right tool for specific machine learning tasks.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Initialize search tool\ndef initialize_search_tool():\n    return TavilySearchResults()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:15.375857Z","iopub.execute_input":"2025-02-17T20:32:15.376128Z","iopub.status.idle":"2025-02-17T20:32:15.379659Z","shell.execute_reply.started":"2025-02-17T20:32:15.376107Z","shell.execute_reply":"2025-02-17T20:32:15.378885Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Initialize retriever tool\ndef initialize_retriever_tool(file_path):\n    \"\"\"\n    This function initializes a retriever tool using a PDF document provided\n    by the `file_path`. The PDF is loaded, processed into smaller chunks, \n    and then converted into a vector representation using FAISS. \n    The retriever tool is then created for searching information related to\n    John Doe's resume.\n\n    Args:\n        file_path (str): Path to the PDF file containing the resume data.\n\n    Returns:\n        RetrieverTool or None: The retriever tool if the file exists and is processed successfully, \n                                or None if the file path is invalid or the file does not exist.\n    \"\"\"\n    \n    # Check if the file path is valid and if the file exists\n    if file_path and os.path.exists(file_path):\n        \n        # Load the PDF file from the specified file path\n        loader = PyPDFLoader(file_path)\n        docs = loader.load()  # Extracts the document content from the PDF\n        \n        # Split the loaded document into smaller chunks for processing\n        # This is done to manage the document size and improve search performance\n        documents = RecursiveCharacterTextSplitter(\n            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n        ).split_documents(docs)\n\n        # Create a FAISS vector from the document chunks using the specified embedding model\n        vector = FAISS.from_documents(documents, EMBEDDING_MODEL)\n        \n        # Create a retriever from the FAISS vector for searching document content\n        retriever = vector.as_retriever()\n\n        # Return the retriever tool, with a description of its purpose\n        return create_retriever_tool(\n            retriever,\n            \"resume_search\",  # Tool name for identifying this retriever\n            \"Search for information about John Doe's resume. For any questions about his qualifications, experience, and skills, use this tool!\",  # Tool description\n        )\n    \n    # Return None if the file doesn't exist or the path is invalid\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:15.895746Z","iopub.execute_input":"2025-02-17T20:32:15.895955Z","iopub.status.idle":"2025-02-17T20:32:15.900536Z","shell.execute_reply.started":"2025-02-17T20:32:15.895936Z","shell.execute_reply":"2025-02-17T20:32:15.899696Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Initialize agent executor with better readability\ndef initialize_agent_executor(llm, tools, logs: bool):\n    \"\"\"\n    Initialize the agent executor with the provided LLM, tools, and logging configuration.\n\n    Args:\n        llm: The language model to be used.\n        tools: The tools to be used by the agent.\n        logs (bool): Whether to enable logging.\n\n    Returns:\n        AgentExecutor: The initialized agent executor.\n    \"\"\"\n    # Pull the prompt for the agent\n    prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n    \n    # Create the agent using the provided LLM and tools\n    agent = create_tool_calling_agent(llm, tools, prompt)\n    \n    # Return the initialized AgentExecutor with the logging configuration\n    return AgentExecutor(agent=agent, tools=tools, verbose=logs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:16.594199Z","iopub.execute_input":"2025-02-17T20:32:16.594558Z","iopub.status.idle":"2025-02-17T20:32:16.599311Z","shell.execute_reply.started":"2025-02-17T20:32:16.594529Z","shell.execute_reply":"2025-02-17T20:32:16.598415Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def initialize_tools(file_path):\n    \"\"\"Initializes and returns LLM, search tool, and retriever tool.\"\"\"\n    llm = initialize_llm()\n    search_tool = initialize_search_tool()\n    retriever_tool = initialize_retriever_tool(file_path)\n    \n    # Ensure tools list does not contain None values\n    tools = [search_tool, retriever_tool] if retriever_tool else [search_tool]\n    \n    return llm, tools","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:17.226064Z","iopub.execute_input":"2025-02-17T20:32:17.226327Z","iopub.status.idle":"2025-02-17T20:32:17.230101Z","shell.execute_reply.started":"2025-02-17T20:32:17.226307Z","shell.execute_reply":"2025-02-17T20:32:17.229455Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def format_chat_history(chat_history):\n    \"\"\"Formats chat history into a string suitable for model input.\"\"\"\n    chat_history = \"\\n\".join(\n        [f\"{msg['role']}: {msg['content']}\" for msg in chat_history]\n    )\n    return chat_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:17.907306Z","iopub.execute_input":"2025-02-17T20:32:17.907590Z","iopub.status.idle":"2025-02-17T20:32:17.911639Z","shell.execute_reply.started":"2025-02-17T20:32:17.907567Z","shell.execute_reply":"2025-02-17T20:32:17.910781Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_agent_response(agent_executor, chat_history):\n    \"\"\"Invokes the agent executor with formatted chat history and returns the response.\"\"\"\n    formatted_history = format_chat_history(chat_history)\n    \n    response_data = agent_executor.invoke(\n        {\"input\": formatted_history},\n        config={\"configurable\": {\"session_id\": \"<foo>\"}},\n    )\n\n    return response_data.get(\"output\", \"\")  # Handle None response safely","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:18.716731Z","iopub.execute_input":"2025-02-17T20:32:18.716940Z","iopub.status.idle":"2025-02-17T20:32:18.720803Z","shell.execute_reply.started":"2025-02-17T20:32:18.716922Z","shell.execute_reply":"2025-02-17T20:32:18.719981Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def stream_response(bot_response, chat_history):\n    \"\"\"Streams the bot's response character by character, yielding updates.\"\"\"\n    global stop_generation\n    \n    chat_history.append({\"role\": \"assistant\", \"content\": \"\"})  # Placeholder for response\n    \n    for char in bot_response:\n        if stop_generation:\n            break\n        chat_history[-1][\"content\"] += char\n        yield \"\", chat_history  # Yield updated chat history for real-time display\n        time.sleep(0.05)  # Simulate a typing delay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:19.397465Z","iopub.execute_input":"2025-02-17T20:32:19.397675Z","iopub.status.idle":"2025-02-17T20:32:19.401485Z","shell.execute_reply.started":"2025-02-17T20:32:19.397656Z","shell.execute_reply":"2025-02-17T20:32:19.400769Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def response(message, chat_history, file_path, max_messages=10):\n    \"\"\"\n    Handles user input, generates a response from the chatbot agent, and streams it character by character.\n\n    Parameters:\n        message (str): The user's input message to the chatbot.\n        chat_history (list): The conversation history as a list of dictionaries.\n        max_messages (int): Maximum number of messages to keep in history.\n\n    Yields:\n        tuple: An empty string (reserved for additional outputs) and the updated chat history.\n    \"\"\"\n    global stop_generation\n    stop_generation = False  # Reset stop flag\n\n    # Initialize LLM and tools\n    llm, tools = initialize_tools(file_path)\n    agent_executor = initialize_agent_executor(llm, tools, False)\n\n    # Limit chat history size\n    chat_history[:] = chat_history[-max_messages:] \n\n    # Add user message to chat history\n    chat_history.append({\"role\": \"user\", \"content\": message})\n\n    # Get bot response\n    bot_response = get_agent_response(agent_executor, chat_history)\n\n    # Stream response character by character\n    yield from stream_response(bot_response, chat_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:20.344253Z","iopub.execute_input":"2025-02-17T20:32:20.344539Z","iopub.status.idle":"2025-02-17T20:32:20.348899Z","shell.execute_reply.started":"2025-02-17T20:32:20.344517Z","shell.execute_reply":"2025-02-17T20:32:20.348058Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def stop_generation_fn():\n  \"\"\"\n  Sets the `stop_generation` flag to True, signaling the response function to halt generation.\n\n  This function is typically triggered by a user action (e.g., pressing a \"Stop\" button).\n  \"\"\"\n  global stop_generation\n  stop_generation = True  # Signal to stop the response generation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:21.618528Z","iopub.execute_input":"2025-02-17T20:32:21.618811Z","iopub.status.idle":"2025-02-17T20:32:21.622424Z","shell.execute_reply.started":"2025-02-17T20:32:21.618789Z","shell.execute_reply":"2025-02-17T20:32:21.621539Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"with gr.Blocks(theme=gr.themes.Origin(radius_size=gr.themes.sizes.radius_md)) as demo:\n  gr.Markdown(\"# Chat Bot\")\n  chatbot = gr.Chatbot(type=\"messages\")\n  msg = gr.Textbox(label=\"Prompt\")\n  with gr.Row():\n      with gr.Column(scale=2):\n          btn = gr.Button(\"Submit\", variant=\"primary\")\n          gr.ClearButton(components=[msg, chatbot], value=\"Clear\", variant=\"secondary\")\n          btn_stop = gr.Button(\"Stop\", variant=\"stop\")\n      with gr.Column(scale=2):\n          uploaded_file = gr.File(type=\"filepath\")\n          \n  btn.click(fn=response, inputs=[msg, chatbot, uploaded_file], outputs=[msg, chatbot])\n  msg.submit(fn=response, inputs=[msg, chatbot, uploaded_file], outputs=[msg, chatbot])\n  btn_stop.click(fn=stop_generation_fn, inputs=[], outputs=[])\n\n  gr.Examples(\n    examples=[\"What are John Doe's skills?\",\n              \"What is your name?\",\n              \"What is the current exchange rate of the dollar in Iran?\"\n             ],\n    inputs=[msg]\n  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:24.319155Z","iopub.execute_input":"2025-02-17T20:32:24.319501Z","iopub.status.idle":"2025-02-17T20:32:24.497565Z","shell.execute_reply.started":"2025-02-17T20:32:24.319471Z","shell.execute_reply":"2025-02-17T20:32:24.496940Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"gr.close_all()\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T20:32:25.705782Z","iopub.execute_input":"2025-02-17T20:32:25.706042Z","iopub.status.idle":"2025-02-17T20:32:28.839393Z","shell.execute_reply.started":"2025-02-17T20:32:25.706021Z","shell.execute_reply":"2025-02-17T20:32:28.838630Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://a96be048c0ae5e5b1a.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://a96be048c0ae5e5b1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"demo.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T19:25:05.903966Z","iopub.execute_input":"2025-02-17T19:25:05.904286Z","iopub.status.idle":"2025-02-17T19:25:06.036267Z","shell.execute_reply.started":"2025-02-17T19:25:05.904265Z","shell.execute_reply":"2025-02-17T19:25:06.035409Z"}},"outputs":[],"execution_count":null}]}